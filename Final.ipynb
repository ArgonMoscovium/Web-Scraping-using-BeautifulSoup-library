{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8a9620-13aa-4f2a-9aa9-6bfd738cd793",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f752d0-fdc1-4ddd-b50e-fb9c151eeee5",
   "metadata": {},
   "source": [
    "\n",
    "Web Scraping is basically extracting data from websites. The purpose is to collect structured data for analysis, storage or other uses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472be1a-4e64-4ca8-81ce-86a7577fb82b",
   "metadata": {},
   "source": [
    "<img src=\"Screenshot 2024-08-10 100637.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be731c-dc10-495b-bc05-d4b76cb265ab",
   "metadata": {},
   "source": [
    "Programs called 'scrapers' access web pages to extract the desired information. These may be commonly used for price monitoring, lead generation, market research, content aggregation etc. \n",
    "\n",
    "- Web scraping is highly relevant as, it helps in \n",
    "    1. Gathering of datasets not availabe through APIs (APIs often have rate limits, they        only provide current  data)\n",
    "    2. Captures data as it appears on websites from HTML pages\n",
    "    3. Gathering training data for training ML models\n",
    "    4. Combine data from multiple sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920458c-6918-454a-8d84-42bb4c43f130",
   "metadata": {},
   "source": [
    "- The project is an attempt to scrape out popular repositories from GitHub. This is done for all the listed topics from the link https://github.com/topics.\n",
    "\n",
    "- ##### Web scraping popular GitHub repositories can be highly beneficial for data analysis and AI/ML enthusiasts as one can,\n",
    "\n",
    "    ##### 1. Identify popular technologies, frameworks and libraries\n",
    "    ##### 2. Gather diverse implementation of techniques\n",
    "    ##### 3. Learn coding standards and project structures\n",
    "    ##### 4. Engage with community, measure user nteractions, stars, forks and contributions and track common dependencies across projects\n",
    "\n",
    "- The process has been carried out ethically, respecting the Terms of Service and by proper rate limiting to aviod overloading GitHub servers. \n",
    "\n",
    "- Some common tools like Python, libraries like Requests and Beautiful Soup have been used. Also Pandas library have been used to create dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3233ba-25c4-4522-8d32-88bca1a1e9d1",
   "metadata": {},
   "source": [
    "##### The project follows the following outline: \n",
    "1. Scrape the list of topics from https://github.com/topics. Obtain a list of such topics\n",
    "2. For each topic get topic title, topic page URL and topic descripton.  \n",
    "3. For each of the topic, get 25 repositories in the topic\n",
    "4. For each of the 25 repositories above, grab the repository name, stars and repository URL.\n",
    "5. Create a CSV file in the following format:\n",
    "Repo_Name,Usernamee,Stars,Repo_URL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a515925-f434-4f94-a576-51c69924bf30",
   "metadata": {},
   "source": [
    "### (A) Obtaining a list of topics from GitHub\n",
    "Use Requests to download page. Use BS4 to parse and extract info. Convert into a Pandas dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d6d26f-7448-4df1-a29f-1beed39e1d7d",
   "metadata": {},
   "source": [
    "Writing a function to download a page,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e204f3-9dfb-4326-8cf9-461d705e2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "# returns 'doc' which contains parsed webpage pointing to list of topics on Github\n",
    "def get_topics_page():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url) # To download webpage\n",
    "    if response.status_code!=200:\n",
    "        raise Exception(f\"Failed to load page {topics_url}\")\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01eeb667-8852-4515-9f63-1e5c90cbf070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = get_topics_page()\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc4339-faf6-4162-a55c-16162a6fe0b6",
   "metadata": {},
   "source": [
    "Helper functions to parse information from the topic page: \n",
    "1. 'get_topic_titles' is used to get list of titles.\n",
    "2. 'get_topic_descrip' to retrieve topic descriptions.\n",
    "3. 'get_topic_urls' to obtain URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b99cc68-1c52-4e61-b1ca-647905b2f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    topic_title_tags = doc.find_all('p', class_=\"f3 lh-condensed mb-0 mt-1 Link--primary\") \n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_descrip(doc):\n",
    "    topic_descrip_tags = doc.find_all('p', class_=\"f5 color-fg-muted mb-0 mt-1\")    \n",
    "    topic_descrip = []\n",
    "    for tag in topic_descrip_tags:\n",
    "        topic_descrip.append(tag.text.strip())\n",
    "    return topic_descrip\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    topic_urls = []\n",
    "    base_url = 'https://github.com'\n",
    "    topic_link_tags=doc.find_all('a', class_=\"no-underline flex-grow-0\")\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag['href'])\n",
    "    return topic_urls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3bd1c-39ad-476b-955f-7d9c119c9b3c",
   "metadata": {},
   "source": [
    "To pick topic titles, 'p' tags with the following class has been picked. Similar process for descriptions and URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cda5b7-8599-449c-8f7d-e62291389626",
   "metadata": {},
   "source": [
    "<img src=\"Screenshot 2024-08-10 113315.png\" style=\"width:500px;height:1000px/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e06da-c2de-4bf5-83a4-e0efd6edda93",
   "metadata": {},
   "source": [
    "Combining all of the above functions into a single function called 'scrape_topics',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9f2ad1-0a25-4325-b5a8-88b02e3ac660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception(f\"Failed to load page {topics_url}\")\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title' : get_topic_titles(doc),\n",
    "        'Description' : get_topic_descrip(doc),\n",
    "        'url' : get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cb01d-c13b-49cc-bf0b-cd97a93e98b2",
   "metadata": {},
   "source": [
    "Creating a Pandas dataframe,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "041aea9d-d846-4134-b447-38bb18f58f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>Description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D refers to the use of three-dimensional grap...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       title                                        Description  \\\n",
       "0         3D  3D refers to the use of three-dimensional grap...   \n",
       "1       Ajax  Ajax is a technique for creating interactive w...   \n",
       "2  Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3        Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4    Android  Android is an operating system built by Google...   \n",
       "\n",
       "                                   url  \n",
       "0         https://github.com/topics/3d  \n",
       "1       https://github.com/topics/ajax  \n",
       "2  https://github.com/topics/algorithm  \n",
       "3      https://github.com/topics/amphp  \n",
       "4    https://github.com/topics/android  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_topics()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2082bee-6e2d-4a72-92be-3be2068b9a02",
   "metadata": {},
   "source": [
    "### (B) Getting top 25 repositories from a topic page \n",
    "These details for each repository will be extracted: \n",
    "1. Repository creater's username\n",
    "2. Name of the repository\n",
    "3. Number of stars (indicates the popularity)\n",
    "4. URL of each "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19507d51-0625-4592-b82c-37072979df52",
   "metadata": {},
   "source": [
    "Using 'get_topic_page' to create a BS4 object. This object will be parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed0c73a9-7667-403b-b34d-e9477bc1dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to load page {topics_url}\")\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef2a8c-e755-45df-a44b-102ed09491e7",
   "metadata": {},
   "source": [
    "Now inside a particular topic (say '3D'). Searching for 'h3' tags, as they contain the repository name ('three.js') as well as the username ('mrdoob')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a17b3d-2492-45c1-bcc5-caa4646ba70f",
   "metadata": {},
   "source": [
    "<img src=\"Screenshot 2024-08-10 115534.png\" style=\"width:800px;height:1600px/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a299dfb-3949-4816-a864-9ceda91dcbfb",
   "metadata": {},
   "source": [
    "The following three functions are discussed,\n",
    "1. 'get_repo_info' function returns the username, repository name, number of stars and URL all at once.\n",
    "2. 'get_topic_repos' function creates a dictionary using the required tags. This then further returns a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f15f9f-5c86-4934-8af0-417e3b19c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tag): \n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url # Returns all reqd info about a repository\n",
    "\n",
    "def get_topic_repos(topic_doc):\n",
    "    repo_tags = topic_doc.find_all('h3', class_ = \"f3 color-fg-muted text-normal lh-condensed\") # Get h3 tags containing repo title, repo URL and username\n",
    "    star_tags = topic_doc.find_all('span', class_ = \"Counter js-social-count\") # Get star tags    \n",
    "    topics_repos_dict = {\n",
    "        'username':[],\n",
    "        'repo_name':[],\n",
    "        'stars':[],\n",
    "        'repo_url':[]\n",
    "    }    \n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topics_repos_dict['username'].append(repo_info[0])\n",
    "        topics_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topics_repos_dict['stars'].append(repo_info[2])\n",
    "        topics_repos_dict['repo_url'].append(repo_info[3])        \n",
    "    return pd.DataFrame(topics_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68dba2-61c7-41f5-b9da-5c8938bfa6ad",
   "metadata": {},
   "source": [
    "The dataframe for topic '3D',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4dc8b-517e-42a4-9063-ac2cd20c6e51",
   "metadata": {},
   "source": [
    "<img src=\"Screenshot 2024-08-10 130742.png\" style=\"width:700px;height:1400px/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ccfd1f-ee9a-4515-997f-6f3c60dfe688",
   "metadata": {},
   "source": [
    "### (C) Creating a CSV file \n",
    "Writing a function to create a .csv file from the above dataframe. And also creating a final composite function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "032ac75b-9fe3-4f56-aacd-6566fcaee6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def scrape_topic(topic_url, path):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"The file {path} already exists. Skipping...\")\n",
    "        return\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50976b29-52ee-4cf0-8602-18fe949482af",
   "metadata": {},
   "source": [
    "'3D.csv' \n",
    "<img src=\"Screenshot 2024-08-10 125340.png\" style=\"width:700px;height:1400px/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ccc58-87b9-4fa6-b123-dca31748f2d4",
   "metadata": {},
   "source": [
    "Finally combining the following functions into one composite function 'scrape_topics_repos' \n",
    "1. Function to ge list of topics 'scrape_topics'\n",
    "2. Function to create a CSV flie for the scraped repository from a topics page 'scrape_topic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6342b0d-b3d5-4ccd-abc9-0ee4c80b5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('Scraping top reopositories  for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae942f0-0253-474d-abf6-879882380b1b",
   "metadata": {},
   "source": [
    "Running the final function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8360c92b-fa6e-43c2-b4d2-2cc2fe3f67b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "Scraping top reopositories  for \"3D\"\n",
      "The file data/3D.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Ajax\"\n",
      "The file data/Ajax.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Algorithm\"\n",
      "The file data/Algorithm.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Amp\"\n",
      "The file data/Amp.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Android\"\n",
      "The file data/Android.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Angular\"\n",
      "The file data/Angular.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Ansible\"\n",
      "The file data/Ansible.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"API\"\n",
      "The file data/API.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Arduino\"\n",
      "The file data/Arduino.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"ASP.NET\"\n",
      "The file data/ASP.NET.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Awesome Lists\"\n",
      "The file data/Awesome Lists.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Amazon Web Services\"\n",
      "The file data/Amazon Web Services.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Azure\"\n",
      "The file data/Azure.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Babel\"\n",
      "The file data/Babel.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Bash\"\n",
      "The file data/Bash.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Bitcoin\"\n",
      "The file data/Bitcoin.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Bootstrap\"\n",
      "The file data/Bootstrap.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Bot\"\n",
      "The file data/Bot.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"C\"\n",
      "The file data/C.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Chrome\"\n",
      "The file data/Chrome.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Chrome extension\"\n",
      "The file data/Chrome extension.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Command-line interface\"\n",
      "The file data/Command-line interface.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Clojure\"\n",
      "The file data/Clojure.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Code quality\"\n",
      "The file data/Code quality.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Code review\"\n",
      "The file data/Code review.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Compiler\"\n",
      "The file data/Compiler.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Continuous integration\"\n",
      "The file data/Continuous integration.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"C++\"\n",
      "The file data/C++.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Cryptocurrency\"\n",
      "The file data/Cryptocurrency.csv already exists. Skipping...\n",
      "Scraping top reopositories  for \"Crystal\"\n",
      "The file data/Crystal.csv already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2681eb-c9af-4bfe-b2e1-431a51e2c89d",
   "metadata": {},
   "source": [
    "### (D) References & Future Work\n",
    "#### Summary\n",
    "Followng areas have been covered: \n",
    "1. Downloading web pages using the requests library, inspecting its HTML source code. \n",
    "2. Parsing parts of a website using Beautiful Soup\n",
    "3. Writing parsed information into CSV files\n",
    "\n",
    "#### References\n",
    "1. https://dorianlazar.medium.com/scraping-medium-with-python-beautiful-soup-3314f898bbf5\n",
    "2. https://www.freecodecamp.org/news/web-scraping-python-tutorial-how-to-scrape-data-from-a-website/\n",
    "3. https://www.freecodecamp.org/news/scraping-wikipedia-articles-with-python/\n",
    "\n",
    "#### Ideas for future work\n",
    "1. Combining each topic's CSVs into a single one.\n",
    "2. Data analysis and exploration after data cleaning/preprocessing\n",
    "3. Time-series analysis to analyze growth of repostories related to certain topics.\n",
    "4. Grouping similar data pints via clustering algorithms\n",
    "5. Set-up automated scraping jobs to regularly update the dataset  \n",
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
